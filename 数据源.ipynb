{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211bb89c",
   "metadata": {},
   "source": [
    "在本节中，我们将介绍如何在 ML 中使用数据源加载数据。除了 Parquet、CSV、JSON 和 JDBC 等一些通用数据源外，我们还为 ML 提供了一些特定的数据源,包括图像数据源、LIBSVM 数据源。\n",
    "图像数据源\n",
    "此图像数据源用于从目录加载图像文件，它可以通过ImageIOJava 库将压缩图像（jpeg、png 等）加载到原始图像表示中。加载的 DataFrame 有一StructType列：“image”，包含存储为图像模式的图像数据。该image列的架构是：\n",
    "origin：（StringType代表图片的文件路径）\n",
    "高度height：（IntegerType图像的高度）\n",
    "宽度width：（IntegerType图像的宽度）\n",
    "nChannels：（IntegerType图像通道数）\n",
    "模式：IntegerType（OpenCV 兼容类型）\n",
    "数据：BinaryType（OpenCV 兼容顺序中的图像字节：大多数情况下是按行 BGR）\n",
    "LIBSVM 数据源\n",
    "此LIBSVM数据源用于从目录加载“libsvm”类型的文件。加载的 DataFrame 有两列：包含以双精度形式存储的标签的标签和包含以向量形式存储的特征向量的特征。列的模式是：\n",
    "标签：（DoubleType代表实例标签）\n",
    "features：（VectorUDT代表特征向量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb33bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------+-----+------+---------+\n",
      "|origin                                                                                                      |width|height|nChannels|\n",
      "+------------------------------------------------------------------------------------------------------------+-----+------+---------+\n",
      "|file:///D:/software/spark/spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/kittens/54893.jpg              |300  |311   |3        |\n",
      "|file:///D:/software/spark/spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/kittens/DP802813.jpg           |199  |313   |3        |\n",
      "|file:///D:/software/spark/spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg|300  |200   |3        |\n",
      "|file:///D:/software/spark/spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/kittens/DP153539.jpg           |300  |296   |3        |\n",
      "+------------------------------------------------------------------------------------------------------------+-----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"image\") \\\n",
    "        .getOrCreate()\n",
    "df = spark.read.format(\"image\").option(\"dropInvalid\", True).load(\"D:\\software\\spark\\spark-3.1.2-bin-hadoop3.2\\data\\mllib\\images\\origin\\kittens\")\n",
    "df.select(\"image.origin\", \"image.width\", \"image.height\",\"image.nChannels\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09cfce98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"libsvm\").load(\"D:\\software\\spark\\spark-3.1.2-bin-hadoop3.2\\data\\mllib\\sample_libsvm_data.txt\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ff79e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(780,[127,128,129...|\n",
      "|  1.0|(780,[158,159,160...|\n",
      "|  1.0|(780,[124,125,126...|\n",
      "|  1.0|(780,[152,153,154...|\n",
      "|  1.0|(780,[151,152,153...|\n",
      "|  0.0|(780,[129,130,131...|\n",
      "|  1.0|(780,[158,159,160...|\n",
      "|  1.0|(780,[99,100,101,...|\n",
      "|  0.0|(780,[154,155,156...|\n",
      "|  0.0|(780,[127,128,129...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"libsvm\").option(\"numFeatures\", \"780\").load(\"D:\\software\\spark\\spark-3.1.2-bin-hadoop3.2\\data\\mllib\\sample_libsvm_data.txt\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2abfe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
