{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ccdb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      " 1.0                   0.055641488407465814  NaN  0.4004714203168137  \n",
      "0.055641488407465814  1.0                   NaN  0.9135958615342522  \n",
      "NaN                   NaN                   1.0  NaN                 \n",
      "0.4004714203168137    0.9135958615342522    NaN  1.0                 \n",
      "Spearman correlation matrix:\n",
      " 1.0                  0.10540925533894598  NaN  0.4                 \n",
      "0.10540925533894598  1.0                  NaN  0.9486832980505139  \n",
      "NaN                  NaN                  1.0  NaN                 \n",
      "0.4                  0.9486832980505139   NaN  1.0                 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.{Matrix, Vectors}\r\n",
       "import org.apache.spark.ml.stat.Correlation\r\n",
       "import org.apache.spark.sql.Row\r\n",
       "data: Seq[org.apache.spark.ml.linalg.Vector] = List((4,[0,3],[1.0,-2.0]), [4.0,5.0,0.0,3.0], [6.0,7.0,0.0,8.0], (4,[0,3],[9.0,1.0]))\r\n",
       "df: org.apache.spark.sql.DataFrame = [features: vector]\r\n",
       "coeff1: org.apache.spark.ml.linalg.Matrix =\r\n",
       "1.0                   0.055641488407465814  NaN  0.4004714203168137\r\n",
       "0.055641488407465814  1.0                   NaN  0.9135958615342522\r\n",
       "NaN                   NaN                   1.0  NaN\r\n",
       "0.4004714203168137    0.9135958615342522    NaN  1.0\r\n",
       "coeff2: org.apache.spark.ml.linalg.Matrix =\r\n",
       "1.0                  0.10540925533894598  NaN  0.4\r\n",
       "0.10540925533894598  1.0                  NaN  0.9486832980505139\r\n",
       "NaN                 ...\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.{Matrix, Vectors}\n",
    "import org.apache.spark.ml.stat.Correlation\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val data = Seq(\n",
    "  Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))),\n",
    "  Vectors.dense(4.0, 5.0, 0.0, 3.0),\n",
    "  Vectors.dense(6.0, 7.0, 0.0, 8.0),\n",
    "  Vectors.sparse(4, Seq((0, 9.0), (3, 1.0)))\n",
    ")\n",
    "\n",
    "val df = data.map(Tuple1.apply).toDF(\"features\")\n",
    "val Row(coeff1: Matrix) = Correlation.corr(df, \"features\").head\n",
    "println(s\"Pearson correlation matrix:\\n $coeff1\")\n",
    "\n",
    "val Row(coeff2: Matrix) = Correlation.corr(df, \"features\", \"spearman\").head\n",
    "println(s\"Spearman correlation matrix:\\n $coeff2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58e0534b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pValues = [0.6872892787909721,0.6822703303362126]\n",
      "degreesOfFreedom [2,3]\n",
      "statistics [0.75,1.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\r\n",
       "import org.apache.spark.ml.stat.ChiSquareTest\r\n",
       "data: Seq[(Double, org.apache.spark.ml.linalg.Vector)] = List((0.0,[0.5,10.0]), (0.0,[1.5,20.0]), (1.0,[1.5,30.0]), (0.0,[3.5,30.0]), (0.0,[3.5,40.0]), (1.0,[3.5,40.0]))\r\n",
       "df: org.apache.spark.sql.DataFrame = [label: double, features: vector]\r\n",
       "chi: org.apache.spark.sql.Row = [[0.6872892787909721,0.6822703303362126],WrappedArray(2, 3),[0.75,1.5]]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.stat.ChiSquareTest\n",
    "\n",
    "val data = Seq(\n",
    "  (0.0, Vectors.dense(0.5, 10.0)),\n",
    "  (0.0, Vectors.dense(1.5, 20.0)),\n",
    "  (1.0, Vectors.dense(1.5, 30.0)),\n",
    "  (0.0, Vectors.dense(3.5, 30.0)),\n",
    "  (0.0, Vectors.dense(3.5, 40.0)),\n",
    "  (1.0, Vectors.dense(3.5, 40.0))\n",
    ")\n",
    "\n",
    "val df = data.toDF(\"label\", \"features\")\n",
    "val chi = ChiSquareTest.test(df, \"features\", \"label\").head\n",
    "println(s\"pValues = ${chi.getAs[Vector](0)}\")\n",
    "println(s\"degreesOfFreedom ${chi.getSeq[Int](1).mkString(\"[\", \",\", \"]\")}\")\n",
    "println(s\"statistics ${chi.getAs[Vector](2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "543715e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.233.1:4041\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1633243486980)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with weight: mean = [3.333333333333333,5.0,6.333333333333333], variance = [2.0,4.5,2.0]\n",
      "without weight: mean = [3.0,4.5,6.0], sum = [2.0,4.5,2.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\r\n",
       "import org.apache.spark.ml.stat.Summarizer\r\n",
       "import spark.implicits._\r\n",
       "import Summarizer._\r\n",
       "data: Seq[(org.apache.spark.ml.linalg.Vector, Double)] = List(([2.0,3.0,5.0],1.0), ([4.0,6.0,7.0],2.0))\r\n",
       "df: org.apache.spark.sql.DataFrame = [features: vector, weight: double]\r\n",
       "meanVal: org.apache.spark.ml.linalg.Vector = [3.333333333333333,5.0,6.333333333333333]\r\n",
       "varianceVal: org.apache.spark.ml.linalg.Vector = [2.0,4.5,2.0]\r\n",
       "meanVal2: org.apache.spark.ml.linalg.Vector = [3.0,4.5,6.0]\r\n",
       "varianceVal2: org.apache.spark.ml.linalg.Vector = [2.0,4.5,2.0]\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.stat.Summarizer\n",
    "import spark.implicits._\n",
    "import Summarizer._\n",
    "val data = Seq(\n",
    "  (Vectors.dense(2.0, 3.0, 5.0), 1.0),\n",
    "  (Vectors.dense(4.0, 6.0, 7.0), 2.0)\n",
    ")\n",
    "\n",
    "val df = data.toDF(\"features\", \"weight\")\n",
    "\n",
    "val (meanVal, varianceVal) = df.select(metrics(\"mean\", \"variance\")\n",
    "  .summary($\"features\", $\"weight\").as(\"summary\"))\n",
    "  .select(\"summary.mean\", \"summary.variance\")\n",
    "  .as[(Vector, Vector)].first()\n",
    "\n",
    "println(s\"with weight: mean = ${meanVal}, variance = ${varianceVal}\")\n",
    "\n",
    "val (meanVal2, varianceVal2) = df.select(mean($\"features\"), variance($\"features\"))\n",
    "  .as[(Vector, Vector)].first()\n",
    "\n",
    "println(s\"without weight: mean = ${meanVal2}, sum = ${varianceVal2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858d3c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
