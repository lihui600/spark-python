{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4750d76",
   "metadata": {},
   "source": [
    "dataframe创建与常规操作\n",
    "DataFrames 为Scala、Java、Python和R 中的结构化数据操作提供了一种特定于领域的语言。\n",
    "如上所述，在 Spark 2.0 中，DataFrame 只是RowScala 和 Java API中s 的Dataset 。与强类型 Scala/Java 数据集附带的“类型转换”相比，这些操作也称为“无类型转换”。\n",
    "这里我们包括一些使用 Datasets 进行结构化数据处理的基本示例："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a5c76",
   "metadata": {},
   "source": [
    "使用SparkSession，应用程序可以从现有的RDD、Hive 表或Spark 数据源创建数据帧。\n",
    "例如，以下内容基于 JSON 文件的内容创建一个 DataFrame："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e9a3f874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import spark.implicits._\r\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f079c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@11edb250\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Spark SQL basic example\")\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ce0ffdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|Michael|null|\n",
      "|   Andy|  30|\n",
      "| Justin|  19|\n",
      "+-------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "peopleDF: org.apache.spark.sql.DataFrame = [name: string, age: string]\r\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//生成一个dataFrame\n",
    "val peopleDF= Seq(\n",
    "     (\"Michael\",\"null\"),\n",
    "     (\"Andy\",\"30\") ,\n",
    "     (\"Justin\",\"19\")\n",
    "    ).toDF(\"name\",\"age\")\n",
    "\n",
    "peopleDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aa72cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.json(\"people.json\")\n",
    "\n",
    "// Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4f91c9b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|     name| age|\n",
      "+---------+----+\n",
      "|  Michael|null|\n",
      "|     Andy|  30|\n",
      "|   Justin|  19|\n",
      "+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//从csv文件加载dataFrame\n",
    "spark.read.option(\"header\", true).csv(\"people.csv\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63ed6e",
   "metadata": {},
   "source": [
    "对DataFrame进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "992868df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//以树结构打印Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b024f72c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//选择多列\n",
    "df.select(df(\"name\"),df(\"age\")+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e790e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//条件过滤\n",
    "df.filter(df(\"age\") > 20 ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e99fbc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//分组聚合\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49aaae62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//排序\n",
    "df.sort(df(\"age\").desc).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68a648a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//多列排序\n",
    "df.sort(df(\"age\").desc, df(\"name\").asc).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d440545e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|username| age|\n",
      "+--------+----+\n",
      "| Michael|null|\n",
      "|    Andy|  30|\n",
      "|  Justin|  19|\n",
      "+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//对列进行重命名\n",
    "df.select(df(\"name\").as(\"username\"),df(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7782a9b",
   "metadata": {},
   "source": [
    "以编程方式运行sql查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45b95e2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\r\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "val sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7c8587ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Person\r\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Person(name: String, age: Long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0a3f9e",
   "metadata": {},
   "source": [
    "创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c5acfc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|Andy| 32|\n",
      "+----+---+\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "caseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]\r\n",
       "path: String = people.json\r\n",
       "peopleDS: org.apache.spark.sql.Dataset[Person] = [age: bigint, name: string]\r\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// Encoders are created for case classes\n",
    "val caseClassDS = (Seq(Person(\"Andy\", 32))).toDS()\n",
    "caseClassDS.show()\n",
    "\n",
    "\n",
    "// Encoders for most common types are automatically provided by importing spark.implicits._\n",
    "//val primitiveDS = Seq(1, 2, 3).toDS()\n",
    "//primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)\n",
    "\n",
    "// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name\n",
    "val path = \"people.json\"\n",
    "val peopleDS = spark.read.json(path).as[Person]\n",
    "peopleDS.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ade8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acb055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afed7cce",
   "metadata": {},
   "source": [
    "与RDD互操作，将现有的 RDD 转换为数据集。Spark SQL 支持两种不同的方法将现有的 RDD 转换为数据集。\n",
    "1.第一种方法使用反射来推断包含特定类型对象的 RDD 的模式。\n",
    "2.通过编程接口，该接口允许您构建架构，然后将其应用于现有 RDD。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d99f8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Person\r\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//import spark.implicits._    ---用于从 RDD 到 DataFrame 的隐式转换\n",
    "case class Person(name: String, age: Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8608efc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 55) (DESKTOP-URD341V executor driver): java.lang.NoClassDefFoundError: Could not initialize class \r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 55) (DESKTOP-URD341V executor driver): java.lang.NoClassDefFoundError: Could not initialize class \r",
      "\tat <init>(<console>:9)\r",
      "\tat <init>(<console>:22)\r",
      "\tat .<init>(<console>:26)\r",
      "\tat .<clinit>(<console>)\r",
      "\tat $anonfun$peopleDF$2(<console>:64)\r",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\r",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\r",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\r",
      "\r",
      "Driver stacktrace:\r",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r",
      "  at scala.Option.foreach(Option.scala:407)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:825)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:784)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:793)\r",
      "  ... 51 elided\r",
      "Caused by: java.lang.NoClassDefFoundError: Could not initialize class \r",
      "  ... 1 more\r",
      ""
     ]
    }
   ],
   "source": [
    "\n",
    "//从文本文件创建 Person 对象的 RDD，将其转换为 Dataframe\n",
    "val peopleDF = spark.sparkContext\n",
    "  .textFile(\"people.txt\")\n",
    "  .map(_.split(\",\"))\n",
    "  .map(attributes => Person(attributes(0), attributes(1).trim.toInt))\n",
    "  .toDF()\n",
    "//将 DataFrame 注册为临时视图(临时视图，视图的生命周期取决于sparksession)，视图可以实现SparkSession能够以编程方式运行SQL查询并返回结果\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "val teenagersDF = spark.sql(\"SELECT name, age FROM people WHERE age BETWEEN 13 AND 19\")\n",
    "teenagersDF.map(teenager => \"Name: \" + teenager(0)).show()\n",
    "teenagersDF.map(teenager => \"Name: \" + teenager.getAs[String](\"name\")).show()\n",
    "//implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]\n",
    "//teenagersDF.map(teenager => teenager.getValuesMap[Any](List(\"name\", \"age\"))).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2c696d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        value|\n",
      "+-------------+\n",
      "|Name: Michael|\n",
      "|   Name: Andy|\n",
      "| Name: Justin|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\r\n",
       "import org.apache.spark.sql.types._\r\n",
       "peopleRDD: org.apache.spark.rdd.RDD[String] = people.txt MapPartitionsRDD[222] at textFile at <console>:56\r\n",
       "schemaString: String = name age\r\n",
       "fields: Array[org.apache.spark.sql.types.StructField] = Array(StructField(name,StringType,true), StructField(age,StringType,true))\r\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(age,StringType,true))\r\n",
       "rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[224] at map at <console>:69\r\n",
       "peopleDF: org.apache.spark.sql.DataFrame = [name: string, age: string]\r\n",
       "results: org.apache.spark.sql.DataFrame = [name: string]\r\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "//将现有RDD转为数据集\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// Create an RDD of Person objects from a text file, convert it to a Dataframe\n",
    "val peopleRDD = spark.sparkContext.textFile(\"people.txt\")\n",
    "\n",
    "// The schema is encoded in a string\n",
    "val schemaString = \"name age\"\n",
    "\n",
    "// Generate the schema based on the string of schema\n",
    "val fields = schemaString.split(\" \")\n",
    "  .map(fieldName => StructField(fieldName, StringType, nullable = true))\n",
    "val schema = StructType(fields)\n",
    "\n",
    "// Convert records of the RDD (people) to Rows\n",
    "val rowRDD = peopleRDD\n",
    "  .map(_.split(\",\"))\n",
    "//.trim获取字段的值转为string\n",
    "  .map(attributes => Row(attributes(0), attributes(1).trim))\n",
    "\n",
    "// Apply the schema to the RDD\n",
    "val peopleDF = spark.createDataFrame(rowRDD, schema)\n",
    "\n",
    "// Creates a temporary view using the DataFrame\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "// SQL can be run over a temporary view created using DataFrames\n",
    "val results = spark.sql(\"SELECT name FROM people\")\n",
    "\n",
    "// The results of SQL queries are DataFrames and support all the normal RDD operations\n",
    "// The columns of a row in the result can be accessed by field index or by field name\n",
    "results.map(attributes => \"Name: \" + attributes(0)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b2b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06816eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
