{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4750d76",
   "metadata": {},
   "source": [
    "dataframe创建与常规操作\n",
    "DataFrames 为Scala、Java、Python和R 中的结构化数据操作提供了一种特定于领域的语言。\n",
    "如上所述，在 Spark 2.0 中，DataFrame 只是RowScala 和 Java API中s 的Dataset 。与强类型 Scala/Java 数据集附带的“类型转换”相比，这些操作也称为“无类型转换”。\n",
    "这里我们包括一些使用 Datasets 进行结构化数据处理的基本示例："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a5c76",
   "metadata": {},
   "source": [
    "使用SparkSession，应用程序可以从现有的RDD、Hive 表或Spark 数据源创建数据帧。\n",
    "例如，以下内容基于 JSON 文件的内容创建一个 DataFrame："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4f079c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3b9d33bd\r\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Spark SQL basic example\")\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1aa72cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\r\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.json(\"people.json\")\n",
    "\n",
    "// Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4f91c9b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|     name| age|\n",
      "+---------+----+\n",
      "|  Michael|null|\n",
      "|     Andy|  30|\n",
      "|   Justin|  19|\n",
      "+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//从csv文件加载dataFrame\n",
    "spark.read.option(\"header\", true).csv(\"file:///C:/Users/72952/Desktop/scala/people.csv\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63ed6e",
   "metadata": {},
   "source": [
    "对DataFrame进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "992868df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//打印模式信息\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b024f72c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//选择多列\n",
    "df.select(df(\"name\"),df(\"age\")+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e790e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//条件过滤\n",
    "df.filter(df(\"age\") > 20 ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e99fbc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//分组聚合\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49aaae62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//排序\n",
    "df.sort(df(\"age\").desc).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68a648a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//多列排序\n",
    "df.sort(df(\"age\").desc, df(\"name\").asc).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d440545e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|username| age|\n",
      "+--------+----+\n",
      "| Michael|null|\n",
      "|    Andy|  30|\n",
      "|  Justin|  19|\n",
      "+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//对列进行重命名\n",
    "df.select(df(\"name\").as(\"username\"),df(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7782a9b",
   "metadata": {},
   "source": [
    "以编程方式运行sql查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45b95e2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\r\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "val sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0a3f9e",
   "metadata": {},
   "source": [
    "创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c5acfc2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "19: error: value toDS is not a member of Seq[Person]\r",
     "output_type": "error",
     "traceback": [
      "<console>:19: error: value toDS is not a member of Seq[Person]\r",
      "       val caseClassDS = (Seq(Person(\"Andy\", 32))).toDS()\r",
      "                                                   ^\r",
      "<console>:24: error: value toDS is not a member of Seq[Int]\r",
      "       val primitiveDS = Seq(1, 2, 3).toDS()\r",
      "                                      ^\r",
      "<console>:29: error: Unable to find encoder for type Person. An implicit Encoder[Person] is needed to store Person instances in a Dataset. Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.\r",
      "       val peopleDS = spark.read.json(path).as[Person]\r",
      "                                              ^\r",
      ""
     ]
    }
   ],
   "source": [
    "case class Person(name: String, age: Long)\n",
    "\n",
    "// Encoders are created for case classes\n",
    "val caseClassDS = (Seq(Person(\"Andy\", 32))).toDS()\n",
    "caseClassDS.show()\n",
    "\n",
    "\n",
    "// Encoders for most common types are automatically provided by importing spark.implicits._\n",
    "val primitiveDS = Seq(1, 2, 3).toDS()\n",
    "primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)\n",
    "\n",
    "// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name\n",
    "val path = \"people.json\"\n",
    "val peopleDS = spark.read.json(path).as[Person]\n",
    "peopleDS.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed7cce",
   "metadata": {},
   "source": [
    "与RDD互操作，将现有的 RDD 转换为数据集。Spark SQL 支持两种不同的方法将现有的 RDD 转换为数据集。\n",
    "1.第一种方法使用反射来推断包含特定类型对象的 RDD 的模式。\n",
    "2.通过编程接口，该接口允许您构建架构，然后将其应用于现有 RDD。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7d99f8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        value|\n",
      "+-------------+\n",
      "|Name: Michael|\n",
      "|   Name: Andy|\n",
      "| Name: Justin|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n",
       "defined class Person\r\n",
       "peopleDF: org.apache.spark.sql.DataFrame = [name: string, age: bigint]\r\n"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//1.For implicit conversions from RDDs to DataFrames\n",
    "//df.createOrReplaceTempView(\"people\")\n",
    "//val sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "//sqlDF.show()\n",
    "import spark.implicits._\n",
    "case class Person(name: String, age: Long)\n",
    "// Create an RDD of Person objects from a text file, convert it to a Dataframe\n",
    "val peopleDF = spark.sparkContext\n",
    "  .textFile(\"people.txt\")\n",
    "  .map(_.split(\",\"))\n",
    "  .map(attributes => Person(attributes(0), attributes(1).trim.toInt))\n",
    "  .toDF()\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "results.map(attributes => \"Name: \" + attributes(0)).show()\n",
    "//val teenagersDF = spark.sql(\"SELECT name, age FROM people WHERE age BETWEEN 13 AND 20\")\n",
    "//teenagersDF.map(teenager => \"Name: \" + teenager(0)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2c696d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        value|\n",
      "+-------------+\n",
      "|Name: Michael|\n",
      "|   Name: Andy|\n",
      "| Name: Justin|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\r\n",
       "import org.apache.spark.sql.types._\r\n",
       "peopleRDD: org.apache.spark.rdd.RDD[String] = people.txt MapPartitionsRDD[119] at textFile at <console>:35\r\n",
       "schemaString: String = name age\r\n",
       "fields: Array[org.apache.spark.sql.types.StructField] = Array(StructField(name,StringType,true), StructField(age,StringType,true))\r\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(age,StringType,true))\r\n",
       "rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[121] at map at <console>:48\r\n",
       "peopleDF: org.apache.spark.sql.DataFrame = [name: string, age: string]\r\n",
       "results: org.apache.spark.sql.DataFrame = [name: string]\r\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//将现有RDD转为数据集\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// Create an RDD of Person objects from a text file, convert it to a Dataframe\n",
    "val peopleRDD = spark.sparkContext.textFile(\"people.txt\")\n",
    "\n",
    "// The schema is encoded in a string\n",
    "val schemaString = \"name age\"\n",
    "\n",
    "// Generate the schema based on the string of schema\n",
    "val fields = schemaString.split(\" \")\n",
    "  .map(fieldName => StructField(fieldName, StringType, nullable = true))\n",
    "val schema = StructType(fields)\n",
    "\n",
    "// Convert records of the RDD (people) to Rows\n",
    "val rowRDD = peopleRDD\n",
    "  .map(_.split(\",\"))\n",
    "  .map(attributes => Row(attributes(0), attributes(1).trim))\n",
    "\n",
    "// Apply the schema to the RDD\n",
    "val peopleDF = spark.createDataFrame(rowRDD, schema)\n",
    "\n",
    "// Creates a temporary view using the DataFrame\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "// SQL can be run over a temporary view created using DataFrames\n",
    "val results = spark.sql(\"SELECT name FROM people\")\n",
    "\n",
    "// The results of SQL queries are DataFrames and support all the normal RDD operations\n",
    "// The columns of a row in the result can be accessed by field index or by field name\n",
    "results.map(attributes => \"Name: \" + attributes(0)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b2b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06816eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
