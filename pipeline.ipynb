{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c1e56c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\r\n",
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\r\n",
       "import org.apache.spark.ml.param.ParamMap\r\n",
       "import org.apache.spark.sql.Row\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96d5e3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.DataFrame = [label: double, features: vector]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training = spark.createDataFrame(Seq((1.0, Vectors.dense(0.0, 1.1, 0.1)),(0.0, Vectors.dense(2.0, 1.0, -1.0)),(0.0,Vectors.dense(2.0, 1.3, 1.0)),(1.0, Vectors.dense(0.0, 1.2, -0.5)))).toDF(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c58b8171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|label|      features|\n",
      "+-----+--------------+\n",
      "|  1.0| [0.0,1.1,0.1]|\n",
      "|  0.0|[2.0,1.0,-1.0]|\n",
      "|  0.0| [2.0,1.3,1.0]|\n",
      "|  1.0|[0.0,1.2,-0.5]|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d7bd80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_d42831fd086a\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c56b678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: lr.type = logreg_4327fda6a3d4\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//设置迭代次数，正则化参数\n",
    "lr.setMaxIter(10).setRegParam(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d2a8aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model1: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid=logreg_4327fda6a3d4, numClasses=2, numFeatures=3\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model1 = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b55c397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 was fit using parameters: {\n",
      "\tlogreg_4327fda6a3d4-aggregationDepth: 2,\n",
      "\tlogreg_4327fda6a3d4-elasticNetParam: 0.0,\n",
      "\tlogreg_4327fda6a3d4-family: auto,\n",
      "\tlogreg_4327fda6a3d4-featuresCol: features,\n",
      "\tlogreg_4327fda6a3d4-fitIntercept: true,\n",
      "\tlogreg_4327fda6a3d4-labelCol: label,\n",
      "\tlogreg_4327fda6a3d4-maxBlockSizeInMB: 0.0,\n",
      "\tlogreg_4327fda6a3d4-maxIter: 10,\n",
      "\tlogreg_4327fda6a3d4-predictionCol: prediction,\n",
      "\tlogreg_4327fda6a3d4-probabilityCol: probability,\n",
      "\tlogreg_4327fda6a3d4-rawPredictionCol: rawPrediction,\n",
      "\tlogreg_4327fda6a3d4-regParam: 0.01,\n",
      "\tlogreg_4327fda6a3d4-standardization: true,\n",
      "\tlogreg_4327fda6a3d4-threshold: 0.5,\n",
      "\tlogreg_4327fda6a3d4-tol: 1.0E-6\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "//model1.parent.extractParamMap查看模型参数\n",
    "println(\"Model 1 was fit using parameters: \" + model1.parent.extractParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad26ff46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramMap: org.apache.spark.ml.param.ParamMap =\r\n",
       "{\r\n",
       "\tlogreg_4327fda6a3d4-maxIter: 30,\r\n",
       "\tlogreg_4327fda6a3d4-regParam: 0.1,\r\n",
       "\tlogreg_4327fda6a3d4-threshold: 0.55\r\n",
       "}\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//paramMap设置模型参数 threshol为阈值\n",
    "val paramMap = ParamMap(lr.maxIter -> 20).put(lr.maxIter, 30).put(lr.regParam -> 0.1, lr.threshold -> 0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec84ab77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramMap2: org.apache.spark.ml.param.ParamMap =\r\n",
       "{\r\n",
       "\tlogreg_4327fda6a3d4-probabilityCol: myProbability\r\n",
       "}\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramMap2 = ParamMap(lr.probabilityCol -> \"myProbability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d683e308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramMapCombined: org.apache.spark.ml.param.ParamMap =\r\n",
       "{\r\n",
       "\tlogreg_4327fda6a3d4-maxIter: 30,\r\n",
       "\tlogreg_4327fda6a3d4-probabilityCol: myProbability,\r\n",
       "\tlogreg_4327fda6a3d4-regParam: 0.1,\r\n",
       "\tlogreg_4327fda6a3d4-threshold: 0.55\r\n",
       "}\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramMapCombined = paramMap ++ paramMap2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eb60edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model2: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid=logreg_4327fda6a3d4, numClasses=2, numFeatures=3\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model2 = lr.fit(training, paramMapCombined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "820f18db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 was fit using parameters: {\n",
      "\tlogreg_4327fda6a3d4-aggregationDepth: 2,\n",
      "\tlogreg_4327fda6a3d4-elasticNetParam: 0.0,\n",
      "\tlogreg_4327fda6a3d4-family: auto,\n",
      "\tlogreg_4327fda6a3d4-featuresCol: features,\n",
      "\tlogreg_4327fda6a3d4-fitIntercept: true,\n",
      "\tlogreg_4327fda6a3d4-labelCol: label,\n",
      "\tlogreg_4327fda6a3d4-maxBlockSizeInMB: 0.0,\n",
      "\tlogreg_4327fda6a3d4-maxIter: 30,\n",
      "\tlogreg_4327fda6a3d4-predictionCol: prediction,\n",
      "\tlogreg_4327fda6a3d4-probabilityCol: myProbability,\n",
      "\tlogreg_4327fda6a3d4-rawPredictionCol: rawPrediction,\n",
      "\tlogreg_4327fda6a3d4-regParam: 0.1,\n",
      "\tlogreg_4327fda6a3d4-standardization: true,\n",
      "\tlogreg_4327fda6a3d4-threshold: 0.55,\n",
      "\tlogreg_4327fda6a3d4-tol: 1.0E-6\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(\"Model 2 was fit using parameters: \" + model2.parent.extractParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7bb736e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test: org.apache.spark.sql.DataFrame = [label: double, features: vector]\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test = spark.createDataFrame(Seq((1.0, Vectors.dense(-1.0, 1.5, 1.3)),(0.0, Vectors.dense(3.0, 2.0, -0.1)),(1.0,Vectors.dense(0.0, 2.2, -1.5)))).toDF(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1310bcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([-1.0,1.5,1.3], 1.0) -> prob=[0.05707304171034022,0.9429269582896598], prediction=1.0\n",
      "([3.0,2.0,-0.1], 0.0) -> prob=[0.9238522311704104,0.07614776882958962], prediction=0.0\n",
      "([0.0,2.2,-1.5], 1.0) -> prob=[0.10972776114779444,0.8902722388522055], prediction=1.0\n"
     ]
    }
   ],
   "source": [
    "model2.transform(test).select(\"features\", \"label\", \"myProbability\", \"prediction\").collect().foreach { case Row(features: Vector,label: Double, prob: Vector, prediction: Double) =>println(s\"($features, $label) -> prob=$prob, prediction=$prediction\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44abb79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\r\n",
       "import org.apache.spark.ml.classification.LogisticRegression\r\n",
       "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\r\n",
       "import org.apache.spark.ml.linalg.Vector\r\n",
       "import org.apache.spark.sql.Row\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3957289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.DataFrame = [id: bigint, text: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training = spark.createDataFrame(Seq((0L, \"a b c d e spark\", 1.0),(1L, \"b d\", 0.0),(2L, \"spark f g h\", 1.0),(3L, \"hadoop mapreduce\",0.0))).toDF(\"id\", \"text\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72880464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_8a86f499f1b9\r\n",
       "hashingTF: org.apache.spark.ml.feature.HashingTF = HashingTF: uid=hashingTF_325b4dc372b4, binary=false, numFeatures=1000\r\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_6b9c0ae13053\r\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_13ebcbc5d346\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "\n",
    "val hashingTF = new HashingTF().setNumFeatures(1000).setInputCol(tokenizer.getOutputCol).setOutputCol(\"features\")\n",
    "\n",
    "val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.001)\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c775425b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_13ebcbc5d346\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5ae00e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write.overwrite().save(\"spark-logistic-regression-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "011697ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test: org.apache.spark.sql.DataFrame = [id: bigint, text: string]\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test = spark.createDataFrame(Seq((4L, \"spark i j k\"),(5L, \"l m n\"),(6L, \"spark hadoop spark\"),(7L, \"apache hadoop\"))).toDF(\"id\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ccdb317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.15964077387874104,0.8403592261212589], prediction=1.0\n",
      "(5, l m n) --> prob=[0.8378325685476612,0.1621674314523388], prediction=0.0\n",
      "(6, spark hadoop spark) --> prob=[0.06926633132976262,0.9307336686702374], prediction=1.0\n",
      "(7, apache hadoop) --> prob=[0.9821575333444208,0.017842466655579203], prediction=0.0\n"
     ]
    }
   ],
   "source": [
    "model.transform(test).select(\"id\", \"text\", \"probability\", \"prediction\").collect().foreach { case Row(id: Long, text: String, prob: Vector,prediction: Double) =>println(s\"($id, $text) --> prob=$prob, prediction=$prediction\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f46094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
