{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2353324b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "计算两个系列数据之间的相关性是统计中的一个常见操作。在spark.ml 我们提供了很多系列中的灵活性，计算两两相关性。\n",
    "目前支持的相关方法是 Pearson 和 Spearman 相关。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99885e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(4,[0,3],[1.0,-2.0])|\n",
      "|   [4.0,5.0,0.0,3.0]|\n",
      "|   [6.0,7.0,0.0,8.0]|\n",
      "| (4,[0,3],[9.0,1.0])|\n",
      "+--------------------+\n",
      "\n",
      "Pearson correlation matrix:\n",
      "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
      "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
      "Spearman correlation matrix:\n",
      "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
      "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "#Correlation 使用指定的方法计算向量的输入数据集的相关矩阵。输出将是一个包含向量列的相关矩阵的 DataFrame。\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"CorrelationExample\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # $example on$\n",
    "    data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "            (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "            (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "            (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "    df = spark.createDataFrame(data, [\"features\"])\n",
    "    df.show()\n",
    "    r1 = Correlation.corr(df, \"features\").head()\n",
    "    print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "    r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "    print(\"Spearman correlation matrix:\\n\" + str(r2[0]))\n",
    "    # $example off$\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df81949a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pValues: [0.6872892787909721,0.6822703303362126]\n",
      "degreesOfFreedom: [2, 3]\n",
      "statistics: [0.75,1.5]\n"
     ]
    }
   ],
   "source": [
    "#假设检验是统计学中一个强大的工具，用于确定结果是否具有统计显着性，无论该结果是否偶然发生。spark.ml目前支持 Pearson 的卡方 (χ2) 独立性测试。\n",
    "#卡方检验\n",
    "#ChiSquareTest对标签的每个特征进行 Pearson 独立性测试。对于每个特征，（特征，标签）对被转换为列联矩阵，计算卡方统计量。所有标签和特征值都必须是分类的。\n",
    "from pyspark.sql import SparkSession\n",
    "# $example on$\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "# $example off$\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ChiSquareTestExample\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # $example on$\n",
    "    data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
    "            (0.0, Vectors.dense(1.5, 20.0)),\n",
    "            (1.0, Vectors.dense(1.5, 30.0)),\n",
    "            (0.0, Vectors.dense(3.5, 30.0)),\n",
    "            (0.0, Vectors.dense(3.5, 40.0)),\n",
    "            (1.0, Vectors.dense(3.5, 40.0))]\n",
    "    df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "    r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "    print(\"pValues: \" + str(r.pValues))\n",
    "    print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "    print(\"statistics: \" + str(r.statistics))\n",
    "    # $example off$\n",
    "\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c845e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|aggregate_metrics(features, weight)|\n",
      "+-----------------------------------+\n",
      "|{[1.0,1.0,1.0], 1}                 |\n",
      "+-----------------------------------+\n",
      "\n",
      "+--------------------------------+\n",
      "|aggregate_metrics(features, 1.0)|\n",
      "+--------------------------------+\n",
      "|{[1.0,1.5,2.0], 2}              |\n",
      "+--------------------------------+\n",
      "\n",
      "+--------------+\n",
      "|mean(features)|\n",
      "+--------------+\n",
      "|[1.0,1.0,1.0] |\n",
      "+--------------+\n",
      "\n",
      "+--------------+\n",
      "|mean(features)|\n",
      "+--------------+\n",
      "|[1.0,1.5,2.0] |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#总结者\n",
    "#我们为Dataframethrough提供了向量列汇总统计信息Summarizer。可用指标是按列计算的最大值、最小值、平均值、总和、方差、标准差和非零值的数量，以及总计数。\n",
    "from pyspark.sql import SparkSession\n",
    "# $example on$\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "# $example off$\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"SummarizerExample\") \\\n",
    "        .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # $example on$\n",
    "    df = sc.parallelize([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
    "                         Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n",
    "\n",
    "    # create summarizer for multiple metrics \"mean\" and \"count\"\n",
    "    summarizer = Summarizer.metrics(\"mean\", \"count\")\n",
    "\n",
    "    # compute statistics for multiple metrics with weight\n",
    "    df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)\n",
    "\n",
    "    # compute statistics for multiple metrics without weight\n",
    "    df.select(summarizer.summary(df.features)).show(truncate=False)\n",
    "\n",
    "    # compute statistics for single metric \"mean\" with weight\n",
    "    df.select(Summarizer.mean(df.features, df.weight)).show(truncate=False)\n",
    "\n",
    "    # compute statistics for single metric \"mean\" without weight\n",
    "    df.select(Summarizer.mean(df.features)).show(truncate=False)\n",
    "    # $example off$\n",
    "\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a601f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
