{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9490803",
   "metadata": {},
   "source": [
    "### RDD的两类操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26110dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = ../data/data.txt MapPartitionsRDD[12] at textFile at <console>:27\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 先读取本地数据，方便之后的操作\n",
    "\n",
    "val lines = sc.textFile(\"../data/data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8fe91fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: Array[String] = Array(\"ELECBOOK CLASSICS \", \"DAVID \", \"COPPERFIELD \", \"\", \"Charles Dickens \")\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 返回几个元素\n",
    "\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d5e502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e03cf74",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfaef2f",
   "metadata": {},
   "source": [
    "#### 1. map(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf23ff1c",
   "metadata": {},
   "source": [
    "每个元素经过函数func计算，得到的结果组成新的数据集（dataset）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f0e7d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lineLengths: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[17] at map at <console>:28\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 将lines中每行的字符串，通过map操作，产生新的行长度的数据集\n",
    "\n",
    "var lineLengths = lines.map(line => line.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6357f9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Array[Int] = Array(17, 6, 12, 0, 16)\r\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineLengths.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e0184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbe48b1c",
   "metadata": {},
   "source": [
    "#### 2. filter(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec61c38",
   "metadata": {},
   "source": [
    "将符合函数func的元素组成新的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "414cb9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linesFiltered: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at filter at <console>:26\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var linesFiltered = lines.filter(line => line.length > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aa10956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[String] = Array(\"ELECBOOK CLASSICS \", \"COPPERFIELD \", \"Charles Dickens \", ELECBOOK CLASSICS, ebc0004. Charles Dickens: David Copperfield)\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesFiltered.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a758ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4f22588",
   "metadata": {},
   "source": [
    "#### 3. flatMap(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f47305",
   "metadata": {},
   "source": [
    "与map()类似，不同之处在于每个输入可以映射到多个输出，函数func应该返回Seq（）序列类型返回值类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "969a3a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linesFlatMap: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at flatMap at <console>:26\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var linesFlatMap = lines.flatMap(line => line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76f7feed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[String] = Array(ELECBOOK, CLASSICS, DAVID, COPPERFIELD, \"\")\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesFlatMap.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de972d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eab26035",
   "metadata": {},
   "source": [
    "#### 4. mapPartitions(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7b415e",
   "metadata": {},
   "source": [
    "与map()类似，不同之处在于，在不同的分区（partition）分别运行RDD，函数func必须是Iterator<T> => Iterator<U>迭代器类型\n",
    "\n",
    "MapPartitions操作的优点：\n",
    "\n",
    "如果是普通的map，比如一个partition中有1万条数据；那么你的function要执行和计算1万次。\n",
    "但是，使用MapPartitions操作之后，一个task（其实就是一个分区）仅仅会执行一次function，function一次接收所有的partition数据。只要执行一次就可以了，性能比较高。\n",
    "    \n",
    "MapPartitions的缺点：\n",
    "    \n",
    "如果是普通的map操作，一次function的执行就处理一条数据；那么如果内存不够用的情况下，比如处理了1千条数据了，那么这个时候内存不够了，那么就可以将已经处理完的1千条数据从内存里面垃圾回收掉，或者用其他方法，腾出空间来吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84254b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linesMapPartition: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[18] at mapPartitions at <console>:26\r\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val linesMapPartition = lines.mapPartitions(x => x.map(x => x.length()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "383b0619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Array[Int] = Array(17, 6, 12, 0, 16)\r\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesMapPartition.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ec325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cf6e3bf",
   "metadata": {},
   "source": [
    "#### 5. mapPartitionsWithIndex(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b46ab",
   "metadata": {},
   "source": [
    "与mapPartitions()类似，不同之处在于，提供了一个整数来代表partition的序号，函数func必须是(Int, Iterator<T>) => Iterator<U>类型https://blog.csdn.net/qq_37050372/article/details/82620737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2867e6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linesMapPartition: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[212] at mapPartitionsWithIndex at <console>:26\r\n"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val linesMapPartition = lines.mapPartitionsWithIndex(\n",
    "    (index: Int, x: Iterator[String]) => x.map(\n",
    "        (x) => x.length()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c49ce69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res55: Array[Int] = Array(18, 6, 12, 0, 16)\r\n"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesMapPartition.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2c0b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66c518ef",
   "metadata": {},
   "source": [
    "#### 6. sample(withReplacement, fraction, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38623931",
   "metadata": {},
   "source": [
    "使用指定的随机数种子，在数据中采样出一部分，可选是否有替换\n",
    "\n",
    "withReplacement：表示抽出样本后是否在放回去，true表示会放回去，这也就意味着抽出的样本可能有重复\n",
    "fraction ：抽出多少，这是一个double类型的参数,0-1之间，eg:0.3表示抽出30%\n",
    "seed：表示一个种子，根据这个seed随机抽取，一般情况下只用前两个参数就可以，这个参数一般用于调试，有时候不知道是程序出问题还是数据出了问题，就可以将这个参数设置为定值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1edf3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linesSample: org.apache.spark.rdd.RDD[String] = PartitionwiseSampledRDD[20] at sample at <console>:29\r\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// withReplacement = true 可以重复选取\n",
    "// fraction = 0.3  抽取30%\n",
    "\n",
    "val linesSample = lines.sample(true, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "557973fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Long = 34171\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad815d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Long = 10133\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesSample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c32218f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Array[String] = Array(\"ELECBOOK CLASSICS \", \"ELECBOOK CLASSICS \", \"Low cost licenses are available. Contact us through our web site \", \". The Electric Book Co 1998 \", \"\", \"\", DAVID, \"\", THE PERSONAL HISTORY AND, \"\")\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesSample.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad03b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d6004c8",
   "metadata": {},
   "source": [
    "#### 7. union(otherDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe35a02",
   "metadata": {},
   "source": [
    "返回两个数据集的联合,\\\\\\s+，将line以空格作为切割符，若line不为空，组合为新的数据集，union函数中A.union(B),A与B联合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c650fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[11] at filter at <console>:26\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var words = lines.flatMap(line => line.split(\"\\\\s+\")).filter(s => !s.isEmpty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cfd0515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: Long = 261269\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9a38e5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordUnion: org.apache.spark.rdd.RDD[String] = UnionRDD[208] at union at <console>:26\r\n"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordUnion = words.union(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6cd63261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res45: Long = 522538\r\n"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordUnion.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3df44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20926691",
   "metadata": {},
   "source": [
    "#### 8. intersection(otherDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd60d6b5",
   "metadata": {},
   "source": [
    "返回两个数据集的交集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "90236714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[201] at filter at <console>:27\r\n"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var words = lines.flatMap(line => line.split(\"\\\\s+\"))\n",
    "                     .filter(s => !s.isEmpty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ec3c7b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordsCount: Long = 261269\r\n"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordsCount = words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "941f82d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordIntersection: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[207] at intersection at <console>:26\r\n"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordIntersection = words.intersection(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "41befef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res44: Long = 26480\r\n"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordIntersection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4239de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cb7e2c0",
   "metadata": {},
   "source": [
    "#### 9. distinct([numPartitions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f0f1e1",
   "metadata": {},
   "source": [
    "去重函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a7813994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[190] at filter at <console>:27\r\n"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var words = lines.flatMap(line => line.split(\"\\\\s+\")) .filter(s => !s.isEmpty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "389cbd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res41: Long = 261269\r\n"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "25fec82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordDistinct: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[199] at distinct at <console>:26\r\n"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordDistinct = words.distinct(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "556c8c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res43: Long = 26480\r\n"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDistinct.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81eaa4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d607280a",
   "metadata": {},
   "source": [
    "#### 10. groupByKey([numPartitions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8d2dd2",
   "metadata": {},
   "source": [
    "在(K, V)数据集上调用时，返回(K, Iterable<V>)类型\n",
    "    \n",
    "**注1：**如果group操作之后是为了聚合（sum，average），则使用reduceByKey或者aggregateByKey可以获得更好的性能；\n",
    "    \n",
    "**注2：**默认情况下，并行取决于父RDD的分区数量（the number of partitions），可以手动设置numPartitions来使用不同的任务（task）数量**（one task for each partition）**；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "540a48cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordPairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[187] at map at <console>:28\r\n"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var wordPairs = lines.flatMap(line => line.split(\"\\\\s+\"))\n",
    "                     .filter(s => !s.isEmpty())\n",
    "                     .map(s => (s, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cacb56fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordGroupByKey: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[188] at groupByKey at <console>:26\r\n"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordGroupByKey = wordPairs.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f27c0edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res40: Array[(String, Iterable[Int])] = Array((Ah!,CompactBuffer(1, 1, 1, 1, 1, 1, 1)), (intimately,CompactBuffer(1)), (bone,CompactBuffer(1)), (twins;,CompactBuffer(1)), (Never,,CompactBuffer(1, 1)))\r\n"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordGroupByKey.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acfc5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec6a4ca1",
   "metadata": {},
   "source": [
    "#### 11. reduceByKey(func, [numPartitions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91d1ce2",
   "metadata": {},
   "source": [
    "在数据集类型（K，V）上调用时，先根据key聚合为（K，（V1，V2，...）），之后对（V1，V2，...）应用reduce函数，生成新的V，最终返回（K，V）类型的数据集，与groupByKey类似，可通过numPartitions配置任务（task）数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e4239e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordPairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[180] at map at <console>:28\r\n"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var wordPairs = lines.flatMap(line => line.split(\"\\\\s+\"))\n",
    "                     .filter(s => !s.isEmpty())\n",
    "                     .map(s => (s, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0d216190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordReduceByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[184] at reduceByKey at <console>:26\r\n"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordReduceByKey = wordPairs.reduceByKey((a, b) => a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8ad8e95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res39: Array[(String, Int)] = Array((Ah!,7), (intimately,1), (bone,1), (twins;,1), (Never,,2))\r\n"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordReduceByKey.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aaad9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d1729ca",
   "metadata": {},
   "source": [
    "#### 12. aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee169f34",
   "metadata": {},
   "source": [
    "在数据集类型（K，V）上调用时，先根据key聚合为（K，（V1，V2，...）），之后通过给定的组合函数（combine functions）和零值（zero value）得出U，最终返回（K，U）类型的数据集，与groupByKey类似，可通过numPartitions配置任务（task）数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "de638aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordPairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[176] at map at <console>:28\r\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var wordPairs = lines.flatMap(line => line.split(\"\\\\s+\"))\n",
    "                     .filter(s => !s.isEmpty())\n",
    "                     .map(s => (s, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bbf49f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "combOp: (a: Int, b: Int)Int\r\n"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combOp(a: Int, b: Int) : Int= {\n",
    "    return a + b\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "04c601ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seqOp: (a: Int, b: Int)Int\r\n"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seqOp(a: Int, b: Int) : Int= {\n",
    "    return a + b\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6db1b017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordAggregateByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[177] at aggregateByKey at <console>:30\r\n"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordAggregateByKey = wordPairs.aggregateByKey(0)(seqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0666ab20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res35: Array[(String, Int)] = Array((Ah!,7), (intimately,1), (bone,1), (twins;,1), (Never,,2))\r\n"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordAggregateByKey.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb4d780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b68f672",
   "metadata": {},
   "source": [
    "#### 13. sortByKey([ascending], [numPartitions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e1065e",
   "metadata": {},
   "source": [
    "在数据集类型（K，V）上调用时，根据boolean类型的ascending参数，选择根据keys升序或者降序排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cbeb194d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordPairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[167] at map at <console>:28\r\n"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var wordPairs = lines.flatMap(line => line.split(\"\\\\s+\"))\n",
    "                     .filter(s => !s.isEmpty())\n",
    "                     .map(s => (s, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3ef3622f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordSortByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[170] at sortByKey at <console>:26\r\n"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var wordSortByKey = wordPairs.sortByKey(true, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "720fd56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res33: Array[(String, Int)] = Array((&c.,1), (&c.,,1), ((0)181,1), ((Are,1), ((But,1))\r\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordSortByKey.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9cb1b4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordSortByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[173] at sortByKey at <console>:26\r\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var wordSortByKey = wordPairs.sortByKey(false, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "299cbc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res34: Array[(String, Int)] = Array((??????it,1), (????would,1), (????was,1), (????to,1), (????that,1))\r\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordSortByKey.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f7b173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb5e5fec",
   "metadata": {},
   "source": [
    "#### 14. join(otherDataset, [numPartitions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bb0204",
   "metadata": {},
   "source": [
    "在数据集类型（K，V）和（K，W）上调用时，返回（K，（V，W））类型的数据集，支持的outer joins包含leftOuterJoin，rightOuterJoin和fullOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "654b2519",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordPairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[40] at map at <console>:27\r\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var wordPairs = lines.flatMap(line => line.split(\" \"))\n",
    "                     .map(s => (s, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "387cba50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordJoin: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[43] at join at <console>:26\r\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordJoin = wordPairs.join(wordPairs, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf115f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Array[(String, (Int, Int))] = Array((weary,,(1,1)), (weary,,(1,1)), (weary,,(1,1)), (weary,,(1,1)), (softness,(1,1)))\r\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordJoin.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d8f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ce7b6c7",
   "metadata": {},
   "source": [
    "#### 15. cogroup(otherDataset,  [numPartitions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c17e74c",
   "metadata": {},
   "source": [
    "在数据集类型（K，V）和（K，W）上调用时，返回（K，（Iterable<V>，Iterable<W>））元组类型的数据集，该操作也叫groupWith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ad1e828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordPairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[45] at map at <console>:27\r\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var wordPairs = lines.flatMap(line => line.split(\" \"))\n",
    "                     .map(s => (s, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c55e422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordCogroup: org.apache.spark.rdd.RDD[(String, (Iterable[Int], Iterable[Int]))] = MapPartitionsRDD[47] at cogroup at <console>:26\r\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var wordCogroup = wordPairs.cogroup(wordPairs, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16850825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Array[(String, (Iterable[Int], Iterable[Int]))] = Array((weary,,(CompactBuffer(1, 1),CompactBuffer(1, 1))), (softness,(CompactBuffer(1),CompactBuffer(1))), (bone,(CompactBuffer(1),CompactBuffer(1))), (mantelpiece??I,(CompactBuffer(1),CompactBuffer(1))), (adoration,(CompactBuffer(1),CompactBuffer(1))))\r\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCogroup.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aa6088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "949f96de",
   "metadata": {},
   "source": [
    "#### 16. cartesian(otherDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae1d8e",
   "metadata": {},
   "source": [
    "在数据集类型T和U上调用时，返回（T，U）数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ab8e480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newDataset: org.apache.spark.rdd.RDD[(String, String)] = CartesianRDD[34] at cartesian at <console>:26\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newDataset = lines.cartesian(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1347d948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[(String, String)] = Array((\"ELECBOOK CLASSICS \",\"ELECBOOK CLASSICS \"), (\"ELECBOOK CLASSICS \",\"DAVID \"), (\"ELECBOOK CLASSICS \",\"COPPERFIELD \"), (\"ELECBOOK CLASSICS \",\"\"), (\"ELECBOOK CLASSICS \",\"Charles Dickens \"))\r\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newDataset.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74908dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e5a5523",
   "metadata": {},
   "source": [
    "#### 17. pipe(command, [envVars])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45641c06",
   "metadata": {},
   "source": [
    "将RDD中的每个分区通过shell命令串联起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed301e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac1e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92f1089c",
   "metadata": {},
   "source": [
    "#### 18. coalesce(numPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b04bd6c",
   "metadata": {},
   "source": [
    "将RDD中的分区数量减少到numPartitions，使得过滤后（filtering down）的数据集操作起来更有效率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6619e2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: org.apache.spark.rdd.RDD[String] = CoalescedRDD[33] at coalesce at <console>:28\r\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f18eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d8a7a8a",
   "metadata": {},
   "source": [
    "#### 19. repartition(numPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858c0f20",
   "metadata": {},
   "source": [
    "重排RDD中的数据，根据numPartitions的值增加或减少partitions，并且自动平衡各个partitions的数据量，使其分布均匀。这个操作通过网络重排所有的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "654dc36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[32] at repartition at <console>:28\r\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb910f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7b7925a",
   "metadata": {},
   "source": [
    "#### 20. repartitionAndSortWithinPartitions(partitioner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6fa911",
   "metadata": {},
   "source": [
    "根据给定的分区器（partitioner）对RDD重新分区（repartition），每个分好的区中，根据记录（records）的keys排序。\n",
    "\n",
    "通过将排序操作下沉到重排操作中，使得比先分区再排序更有效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659be63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc2c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8c88402",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cddd045",
   "metadata": {},
   "source": [
    "#### 1. reduce(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cc2dec",
   "metadata": {},
   "source": [
    "用函数func聚合数据集中的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e83bf69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lineLengths: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at map at <console>:26\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var lineLengths = lines.map(line => line.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08c4c62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "totalLength: Int = 1435059\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val totalLength = lineLengths.reduce((x, y) => x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c9659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bf4291c",
   "metadata": {},
   "source": [
    "#### 2. collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b468db0",
   "metadata": {},
   "source": [
    "返回数据集中的所有元素，通常是经过filter或者其他操作，使得数据集足够小的时候使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "233e7420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[String] = Array(\"ELECBOOK CLASSICS \", \"DAVID \", \"COPPERFIELD \", \"\", \"Charles Dickens \", \"\", \"\", \"\r\n",
       "\", ELECBOOK CLASSICS, \"\", \"\", ebc0004. Charles Dickens: David Copperfield, \"\", \"\", This file is free for individual use only. It must not be altered or resold., Organisations wishing to use it must first obtain a licence., \"Low cost licenses are available. Contact us through our web site \", \"\", \"\", \". The Electric Book Co 1998 \", \"\", \"The Electric Book Company Ltd \", \"\", \"20 Cambridge Drive, London SE12 8AJ, UK \", \"+44 (0)181 488 3872 www.elecbook.com \", \"\", \"\r\n",
       "\", DAVID, COPPERFIELD, \"\", \"\", THE PERSONAL HISTORY AND, EXPERIENCE OF DAVID, COPPERFIELD THE YOUNGER, \"\", \"\", CHARLES DICKENS, \"\", \"\", AFFECTIONATELY INSCRIBED TO, THE HON. Mr. AND Mrs. RICHARD WATSON,, OF ROCKINGHAM, N...\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fa629a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c7abd21",
   "metadata": {},
   "source": [
    "#### 3. count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e94a9",
   "metadata": {},
   "source": [
    "返回数据集中元素的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3c4af3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Long = 34171\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05414f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e85563d8",
   "metadata": {},
   "source": [
    "#### 4. first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd9b19",
   "metadata": {},
   "source": [
    "返回数据集中第一个元素，和take(1)类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "082f4049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: String = \"ELECBOOK CLASSICS \"\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9fdc30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d59ca43a",
   "metadata": {},
   "source": [
    "#### 5. take(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea70490",
   "metadata": {},
   "source": [
    "返回数据集中包含前n个元素的数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e0b84d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Array[String] = Array(\"ELECBOOK CLASSICS \", \"DAVID \", \"COPPERFIELD \", \"\", \"Charles Dickens \")\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca347dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d100c6e9",
   "metadata": {},
   "source": [
    "#### 6. takeSample(withReplacement, num, [seed])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccbd9e1",
   "metadata": {},
   "source": [
    "返回数据集中随机的num个元素,withReplacement表示元素是否可以被多次抽样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4e1f650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Array[String] = Array(\"\", \"??Yes. If you think so,?? said I. \", \"have time gradually to make her familiar with my hopes, as \", \"Charles Dickens ElecBook Classics \", \"are so exactly what you used to be, with that agreeable face, and \")\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.takeSample(false, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "313bd357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: Array[String] = Array(\"She next swept up the crumbs with a little broom (putting on a \", \"Agnes, netting a purse. \", \"On somebody??s motion, we resolved to go downstairs to the \", \"that there was a tradition in the Commons that he lived principally \", \"without explanation: she waving her hand and smiling farewell \")\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.takeSample(false, 5, 777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e74617e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Array[String] = Array(\"a charm, from the moment of his being usefully employed; and if \", \"??He??s coming to himself,?? said Peggotty. \", \"\", \"every conceivable variety of discouraging construction on all that \", \"\")\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.takeSample(true, 5, 777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0235c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2175d76d",
   "metadata": {},
   "source": [
    "#### 7. takeOrdered(n, [ordering])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd6d0cd",
   "metadata": {},
   "source": [
    "返回RDD中的前n个元素，可选使用自然排序或者自定义排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de8d04bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Array[String] = Array(\"\", \"\", \"\", \"\", \"\")\r\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.takeOrdered(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d5c67eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MyOrdering\r\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyOrdering extends Ordering[String] {\n",
    "    override def compare(x: String, y: String): Int = {\n",
    "        return y.length() - x.length()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6eca350a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order: MyOrdering = MyOrdering@752d291d\r\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var order = new MyOrdering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "efd960e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res25: Array[String] = Array(\"driveller next day, on receipt of a little lace-edged sheet of notepaper, ??Favoured by papa. To remind??; and passed the intervening \", \"It was a likely place to sell a jacket in; for the dealers in secondhand clothes were numerous, and were, generally speaking, on the \", \"I did. But I felt she was a little impracticable. It damped my newborn ardour, to find that ardour so difficult of communication to \", \"out upon me from a doorway, and whispering the word ??Marriagelicence?? in my ear, was with great difficulty prevented from taking \", \"I learned, was the presiding judge. In the space within the horseshoe, lower than these, that is to say, on about the level of the \")\r\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.takeOrdered(5)(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9fb236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f67a3d72",
   "metadata": {},
   "source": [
    "#### 8. saveAsTextFile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd5f40",
   "metadata": {},
   "source": [
    "将数据集中的元素保存到文件系统，文件系统可以是本地磁盘、HDFS或者其他Hadoop支持的文件系统，Spark调用每个元素的toString()方法，将数据集保存成一个或者多个文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81116090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[String] = Array(\"ELECBOOK CLASSICS \", \"DAVID \", \"COPPERFIELD \", \"\", \"Charles Dickens \")\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75fb2f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.coalesce(1,true).saveAsTextFile(\"../data/out/a.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ab77b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "761e3bed",
   "metadata": {},
   "source": [
    "#### 9. saveAsSequenceFile(path)\n",
    "(Java and Scala)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a3c463",
   "metadata": {},
   "source": [
    "将数据集中的元素保存为Hadoop SequenceFile，要保存的RDDs中的key-value对要实现Hadoop的Writable接口，在scala中，基本类型例如：Int、Double、String等可以隐式转换成Writable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47c0e6be",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "28: error: value saveAsSequenceFile is not a member of org.apache.spark.rdd.RDD[String]\r",
     "output_type": "error",
     "traceback": [
      "<console>:28: error: value saveAsSequenceFile is not a member of org.apache.spark.rdd.RDD[String]\r",
      "       lines.saveAsSequenceFile(\"../data/out/a.txt\")\r",
      "             ^\r",
      ""
     ]
    }
   ],
   "source": [
    "lines.saveAsSequenceFile(\"../data/out/a.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af585b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d845c877",
   "metadata": {},
   "source": [
    "#### 10. saveAsObjectFile(path)\n",
    "(Java and Scala)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe90b1a8",
   "metadata": {},
   "source": [
    "将数据集中的元素用Java序列化方法保存下来，之后可以用SparkContext.objectFile()加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56455f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.saveAsObjectFile(\"../data/out/a.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3587dd",
   "metadata": {},
   "source": [
    "#### 11. countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbbb8ba",
   "metadata": {},
   "source": [
    "仅适用于（K，V）类型的RDDs，返回一个哈希表（hashmap）存储（K，Int），Int代表每个key出现的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4705067b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordPairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[26] at map at <console>:27\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var wordPairs = lines.flatMap(line => line.split(\" \"))\n",
    "                     .map(s => (s, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "191c501b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linesCountByKey: scala.collection.Map[String,Long] = Map(professed -> 2, chary -> 1, strange!?? -> 1, herself. -> 14, face??on??y -> 1, incident -> 5, visit??just -> 1, serious -> 15, brink -> 2, pretensions. -> 2, blushes), -> 1, youthful -> 17, sinister -> 2, comply -> 1, aunt? -> 1, ebb -> 2, breaks -> 3, ??is -> 26, savour, -> 1, sneezed -> 1, forgotten -> 15, yonder??? -> 1, precious -> 15, leer -> 1, stop, -> 3, disgraceful.?? -> 1, compliment -> 5, frostily, -> 1, perhaps??something -> 1, might,?? -> 1, hourly -> 1, got, -> 4, distressed, -> 2, door-post -> 2, respecting -> 8, seclusion, -> 1, ??CANTERBURY, -> 1, evermore.?? -> 1, friendless -> 1, sputtering -> 2, Mell; -> 1, lover -> 2, of. -> 19, lead. -> 1, malignant -> 1, admirably: -> 1, speaker -> 2, ever? -> 1, human, -> 1...\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var linesCountByKey = wordPairs.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c64968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68f2f37c",
   "metadata": {},
   "source": [
    "#### 12. foreach(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7284e1fa",
   "metadata": {},
   "source": [
    "对数据集中的每个元素应用函数func，这通常是更新累加器（Accumulator）或者和外部存储系统交互的副作用\n",
    "\n",
    "**注：**在foreach()外修改除累加器之外的变量，可能会有无法预料的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "108dad4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELECBOOK CLASSICS DAVID COPPERFIELD CHARLES DICKENS "
     ]
    },
    {
     "data": {
      "text/plain": [
       "linesForeach: Unit = ()\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var linesForeach = lines.take(5).foreach(line => print(line.toUpperCase()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc487b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
